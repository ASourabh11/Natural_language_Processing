{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice notebook for dataprocessing in NLP : Lemmatizing and stopward removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stopwords is dictionary of common words such as and,i,you,so ; which do not add any value during the sentimental analysis. Therefore we remove these words from corpus**\n",
    "\n",
    "**Lemmatizing is the process in which if come, coming, came word repeats in sentence then it will replace it with \"come\". So it checks repeating words and replace it with most with most appropriate word\"**\n",
    "\n",
    "**The difference between Stemming and Lemmatizing is, lemmatizing will give you meaningfull word and stemming will give you only stem of the word without any meaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing WordNetLemmatizer for Lemmatizing and stopwards to remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizing = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Information about NLP from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus= \"\"\"Symbolic NLP (1950s - early 1990s)\n",
    "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: \n",
    "Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), \n",
    "the computer emulates natural language understanding (or other NLP tasks) \n",
    "by applying those rules to the data it is confronted with.\n",
    "\n",
    "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.\n",
    "The authors claimed that within three or five years, machine translation would be a solved problem.\n",
    "[2] However, real progress was much slower, and after the ALPAC report in 1966, \n",
    "which found that ten-year-long research had failed to fulfill the expectations,\n",
    "funding for machine translation was dramatically reduced. \n",
    "Little further research in machine translation was conducted until \n",
    "the late 1980s when the first statistical machine translation systems were developed.\n",
    "1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, \n",
    "a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, \n",
    "a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. \n",
    "Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. \n",
    "When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example,\n",
    "responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
    "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", \n",
    "which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), \n",
    "SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first many chatterbots were written (e.g., PARRY).\n",
    "1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. \n",
    "Focus areas of the time included research on rule-based parsing \n",
    "(e.g., the development of HPSG as a computational operationalization of generative grammar), \n",
    "morphology (e.g., two-level morphology[3]), semantics (e.g., Lesk algorithm),\n",
    "reference (e.g., within Centering Theory[4]) and other areas of natural language understanding \n",
    "(e.g., in the Rhetorical Structure Theory). Other lines of research were continued, \n",
    "e.g., the development of chatterbots with Racter and Jabberwacky. \n",
    "An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of \n",
    "quantitative evaluation in this period.[5]Statistical NLP (1990s - 2010s)\n",
    "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. \n",
    "Starting in the late 1980s, however, there was a revolution in natural language processing with the \n",
    "introduction of machine learning algorithms for language processing. \n",
    "This was due to both the steady increase in computational power (see Moore's law)\n",
    "and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), \n",
    "whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
    "\n",
    "1990s: Many of the notable early successes on statistical methods in NLP occurred \n",
    "in the field of machine translation, due especially to work at IBM Research. \n",
    "These systems were able to take advantage of existing multilingual textual corpora that had been produced by \n",
    "the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings\n",
    "into all official languages of the corresponding systems of government. \n",
    "However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, \n",
    "which was (and often continues to be) a major limitation in the success of these systems. \n",
    "As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
    "2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the \n",
    "mid-1990s.\n",
    "Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. \n",
    "Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a \n",
    "combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning,\n",
    "and typically produces less accurate results for a given amount of input data. \n",
    "However, there is an enormous amount of non-annotated data available \n",
    "(including, among other things, the entire content of the World Wide Web), \n",
    "which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\n",
    "Neural NLP (present)In the 2010s, \n",
    "representation learning and deep neural network-style machine learning methods became widespread in natural language processing,\n",
    "due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in \n",
    "many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting entire paragraph into list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenses = nltk.sent_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatizing the sentences and removing the stopwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. Converting each sentense into list of word \n",
    "2. Keeping only word which is not in stopword dataset\n",
    "3. Lemmatizing the remaining word\n",
    "4. Joining all words again to form a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentenses)):\n",
    "    words_in_sentenses = nltk.word_tokenize(sentenses[i])\n",
    "    words_in_sentenses = [lemmatizing.lemmatize(word) for word in words_in_sentenses if word not in set(stopwords.words('english'))]\n",
    "    sentenses[i] = ' '.join(words_in_sentenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Symbolic NLP ( 1950s - early 1990s ) The premise symbolic NLP well-summarized John Searle 's Chinese room experiment : Given collection rule ( e.g . , Chinese phrasebook , question matching answer ) , computer emulates natural language understanding ( NLP task ) applying rule data confronted .\",\n",
       " '1950s : The Georgetown experiment 1954 involved fully automatic translation sixty Russian sentence English .',\n",
       " 'The author claimed within three five year , machine translation would solved problem .',\n",
       " '[ 2 ] However , real progress much slower , ALPAC report 1966 , found ten-year-long research failed fulfill expectation , funding machine translation dramatically reduced .',\n",
       " 'Little research machine translation conducted late 1980s first statistical machine translation system developed .',\n",
       " '1960s : Some notably successful natural language processing system developed 1960s SHRDLU , natural language system working restricted `` block world `` restricted vocabulary , ELIZA , simulation Rogerian psychotherapist , written Joseph Weizenbaum 1964 1966 .',\n",
       " 'Using almost information human thought emotion , ELIZA sometimes provided startlingly human-like interaction .',\n",
       " 'When `` patient `` exceeded small knowledge base , ELIZA might provide generic response , example , responding `` My head hurt `` `` Why say head hurt ? `` .',\n",
       " '1970s : During 1970s , many programmer began write `` conceptual ontology `` , structured real-world information computer-understandable data .',\n",
       " 'Examples MARGIE ( Schank , 1975 ) , SAM ( Cullingford , 1978 ) , PAM ( Wilensky , 1978 ) , TaleSpin ( Meehan , 1976 ) , QUALM ( Lehnert , 1977 ) , Politics ( Carbonell , 1979 ) , Plot Units ( Lehnert 1981 ) .',\n",
       " 'During time , first many chatterbots written ( e.g . , PARRY ) .',\n",
       " '1980s : The 1980s early 1990s mark hey-day symbolic method NLP .',\n",
       " 'Focus area time included research rule-based parsing ( e.g . , development HPSG computational operationalization generative grammar ) , morphology ( e.g . , two-level morphology [ 3 ] ) , semantics ( e.g . , Lesk algorithm ) , reference ( e.g . , within Centering Theory [ 4 ] ) area natural language understanding ( e.g . , Rhetorical Structure Theory ) .',\n",
       " 'Other line research continued , e.g . , development chatterbots Racter Jabberwacky .',\n",
       " 'An important development ( eventually led statistical turn 1990s ) rising importance quantitative evaluation period .',\n",
       " '[ 5 ] Statistical NLP ( 1990s - 2010s ) Up 1980s , natural language processing system based complex set hand-written rule .',\n",
       " 'Starting late 1980s , however , revolution natural language processing introduction machine learning algorithm language processing .',\n",
       " \"This due steady increase computational power ( see Moore 's law ) gradual lessening dominance Chomskyan theory linguistics ( e.g .\",\n",
       " 'transformational grammar ) , whose theoretical underpinnings discouraged sort corpus linguistics underlies machine-learning approach language processing .',\n",
       " '[ 6 ] 1990s : Many notable early success statistical method NLP occurred field machine translation , due especially work IBM Research .',\n",
       " 'These system able take advantage existing multilingual textual corpus produced Parliament Canada European Union result law calling translation governmental proceeding official language corresponding system government .',\n",
       " 'However , system depended corpus specifically developed task implemented system , ( often continues ) major limitation success system .',\n",
       " 'As result , great deal research gone method effectively learning limited amount data .',\n",
       " '2000s : With growth web , increasing amount raw ( unannotated ) language data become available since mid-1990s .',\n",
       " 'Research thus increasingly focused unsupervised semi-supervised learning algorithm .',\n",
       " 'Such algorithm learn data hand-annotated desired answer using combination annotated non-annotated data .',\n",
       " 'Generally , task much difficult supervised learning , typically produce le accurate result given amount input data .',\n",
       " 'However , enormous amount non-annotated data available ( including , among thing , entire content World Wide Web ) , often make inferior result algorithm used low enough time complexity practical .',\n",
       " 'Neural NLP ( present ) In 2010s , representation learning deep neural network-style machine learning method became widespread natural language processing , due part flurry result showing technique [ 7 ] [ 8 ] achieve state-of-the-art result many natural language task , example language modeling , [ 9 ] parsing , [ 10 ] [ 11 ] many others .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
